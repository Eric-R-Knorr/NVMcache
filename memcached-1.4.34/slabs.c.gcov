        -:    0:Source:slabs.c
        -:    0:Graph:slabs.gcno
        -:    0:Data:slabs.gcda
        -:    0:Runs:183
        -:    1:/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */
        -:    2:/*
        -:    3: * Slabs memory allocation, based on powers-of-N. Slabs are up to 1MB in size
        -:    4: * and are divided into chunks. The chunk sizes start off at the size of the
        -:    5: * "item" structure plus space for a small key and value. They increase by
        -:    6: * a multiplier factor from there, up to half the maximum slab size. The last
        -:    7: * slab size is always 1MB, since that's the maximum item size allowed by the
        -:    8: * memcached protocol.
        -:    9: */
        -:   10:#include "memcached.h"
        -:   11:#include <sys/stat.h>
        -:   12:#include <sys/socket.h>
        -:   13:#include <sys/resource.h>
        -:   14:#include <fcntl.h>
        -:   15:#include <netinet/in.h>
        -:   16:#include <errno.h>
        -:   17:#include <stdlib.h>
        -:   18:#include <stdio.h>
        -:   19:#include <string.h>
        -:   20:#include <signal.h>
        -:   21:#include <assert.h>
        -:   22:#include <pthread.h>
        -:   23:
        -:   24://#define DEBUG_SLAB_MOVER
        -:   25:/* powers-of-N allocation structures */
        -:   26:
        -:   27:typedef struct {
        -:   28:    unsigned int size;      /* sizes of items */
        -:   29:    unsigned int perslab;   /* how many items per slab */
        -:   30:
        -:   31:    void *slots;           /* list of item ptrs */
        -:   32:    unsigned int sl_curr;   /* total free items in list */
        -:   33:
        -:   34:    unsigned int slabs;     /* how many slabs were allocated for this class */
        -:   35:
        -:   36:    void **slab_list;       /* array of slab pointers */
        -:   37:    unsigned int list_size; /* size of prev array */
        -:   38:
        -:   39:    size_t requested; /* The number of requested bytes */
        -:   40:} slabclass_t;
        -:   41:
        -:   42:static slabclass_t slabclass[MAX_NUMBER_OF_SLAB_CLASSES];
        -:   43:static size_t mem_limit = 0;
        -:   44:static size_t mem_malloced = 0;
        -:   45:/* If the memory limit has been hit once. Used as a hint to decide when to
        -:   46: * early-wake the LRU maintenance thread */
        -:   47:static bool mem_limit_reached = false;
        -:   48:static int power_largest;
        -:   49:
        -:   50:static void *mem_base = NULL;
        -:   51:static void *mem_current = NULL;
        -:   52:static size_t mem_avail = 0;
        -:   53:
        -:   54:/**
        -:   55: * Access to the slab allocator is protected by this lock
        -:   56: */
        -:   57:static pthread_mutex_t slabs_lock = PTHREAD_MUTEX_INITIALIZER;
        -:   58:static pthread_mutex_t slabs_rebalance_lock = PTHREAD_MUTEX_INITIALIZER;
        -:   59:
        -:   60:/*
        -:   61: * Forward Declarations
        -:   62: */
        -:   63:static int do_slabs_newslab(const unsigned int id);
        -:   64:static void *memory_allocate(size_t size);
        -:   65:static void do_slabs_free(void *ptr, const size_t size, unsigned int id);
        -:   66:
        -:   67:/* Preallocate as many slab pages as possible (called from slabs_init)
        -:   68:   on start-up, so users don't get confused out-of-memory errors when
        -:   69:   they do have free (in-slab) space, but no space to make new slabs.
        -:   70:   if maxslabs is 18 (POWER_LARGEST - POWER_SMALLEST + 1), then all
        -:   71:   slab types can be made.  if max memory is less than 18 MB, only the
        -:   72:   smaller ones will be made.  */
        -:   73:static void slabs_preallocate (const unsigned int maxslabs);
        -:   74:
        -:   75:/*
        -:   76: * Figures out which slab class (chunk size) is required to store an item of
        -:   77: * a given size.
        -:   78: *
        -:   79: * Given object size, return id to use when allocating/freeing memory for object
        -:   80: * 0 means error: can't store such a large object
        -:   81: */
        -:   82:
   170934:   83:unsigned int slabs_clsid(const size_t size) {
   170934:   84:    int res = POWER_SMALLEST;
        -:   85:
   170934:   86:    if (size == 0 || size > settings.item_size_max)
        -:   87:        return 0;
  1672565:   88:    while (size > slabclass[res].size)
  1520976:   89:        if (res++ == power_largest)     /* won't fit in the biggest slab */
    19337:   90:            return power_largest;
   151589:   91:    return res;
        -:   92:}
        -:   93:
        -:   94:/**
        -:   95: * Determines the chunk sizes and initializes the slab class descriptors
        -:   96: * accordingly.
        -:   97: */
       89:   98:void slabs_init(const size_t limit, const double factor, const bool prealloc, const uint32_t *slab_sizes) {
       89:   99:    int i = POWER_SMALLEST - 1;
       89:  100:    unsigned int size = sizeof(item) + settings.chunk_size;
        -:  101:
       89:  102:    mem_limit = limit;
        -:  103:
       89:  104:    if (prealloc) {
        -:  105:        /* Allocate everything in a big chunk with malloc */
    #####:  106:        mem_base = malloc(mem_limit);
    #####:  107:        if (mem_base != NULL) {
    #####:  108:            mem_current = mem_base;
    #####:  109:            mem_avail = mem_limit;
        -:  110:        } else {
    #####:  111:            fprintf(stderr, "Warning: Failed to allocate requested memory in"
        -:  112:                    " one large chunk.\nWill allocate in smaller chunks\n");
        -:  113:        }
        -:  114:    }
        -:  115:
       89:  116:    memset(slabclass, 0, sizeof(slabclass));
        -:  117:
     3613:  118:    while (++i < MAX_NUMBER_OF_SLAB_CLASSES-1) {
     3613:  119:        if (slab_sizes != NULL) {
    #####:  120:            if (slab_sizes[i-1] == 0)
        -:  121:                break;
        -:  122:            size = slab_sizes[i-1];
     3613:  123:        } else if (size >= settings.slab_chunk_size_max / factor) {
        -:  124:            break;
        -:  125:        }
        -:  126:        /* Make sure items are always n-byte aligned */
     3524:  127:        if (size % CHUNK_ALIGN_BYTES)
     2395:  128:            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);
        -:  129:
     3524:  130:        slabclass[i].size = size;
     3524:  131:        slabclass[i].perslab = settings.slab_page_size / slabclass[i].size;
     3524:  132:        if (slab_sizes == NULL)
     3524:  133:            size *= factor;
     3524:  134:        if (settings.verbose > 1) {
     3736:  135:            fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
        -:  136:                    i, slabclass[i].size, slabclass[i].perslab);
        -:  137:        }
        -:  138:    }
        -:  139:
       89:  140:    power_largest = i;
       89:  141:    slabclass[power_largest].size = settings.slab_chunk_size_max;
       89:  142:    slabclass[power_largest].perslab = settings.slab_page_size / settings.slab_chunk_size_max;
       89:  143:    if (settings.verbose > 1) {
        3:  144:        fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
        -:  145:                i, slabclass[i].size, slabclass[i].perslab);
        -:  146:    }
        -:  147:
        -:  148:    /* for the test suite:  faking of how much we've already malloc'd */
        -:  149:    {
       89:  150:        char *t_initial_malloc = getenv("T_MEMD_INITIAL_MALLOC");
       89:  151:        if (t_initial_malloc) {
        1:  152:            mem_malloced = (size_t)atol(t_initial_malloc);
        -:  153:        }
        -:  154:
        -:  155:    }
        -:  156:
       89:  157:    if (prealloc) {
    #####:  158:        slabs_preallocate(power_largest);
        -:  159:    }
       89:  160:}
        -:  161:
    #####:  162:static void slabs_preallocate (const unsigned int maxslabs) {
    #####:  163:    int i;
    #####:  164:    unsigned int prealloc = 0;
        -:  165:
        -:  166:    /* pre-allocate a 1MB slab in every size class so people don't get
        -:  167:       confused by non-intuitive "SERVER_ERROR out of memory"
        -:  168:       messages.  this is the most common question on the mailing
        -:  169:       list.  if you really don't want this, you can rebuild without
        -:  170:       these three lines.  */
        -:  171:
    #####:  172:    for (i = POWER_SMALLEST; i < MAX_NUMBER_OF_SLAB_CLASSES; i++) {
    #####:  173:        if (++prealloc > maxslabs)
        -:  174:            return;
    #####:  175:        if (do_slabs_newslab(i) == 0) {
    #####:  176:            fprintf(stderr, "Error while preallocating slab memory!\n"
        -:  177:                "If using -L or other prealloc options, max memory must be "
        -:  178:                "at least %d megabytes.\n", power_largest);
    #####:  179:            exit(1);
        -:  180:        }
        -:  181:    }
        -:  182:
        -:  183:}
        -:  184:
      834:  185:static int grow_slab_list (const unsigned int id) {
      834:  186:    slabclass_t *p = &slabclass[id];
      834:  187:    if (p->slabs == p->list_size) {
      182:  188:        size_t new_size =  (p->list_size != 0) ? p->list_size * 2 : 16;
      182:  189:        void *new_list = realloc(p->slab_list, new_size * sizeof(void *));
      182:  190:        if (new_list == 0) return 0;
      182:  191:        p->list_size = new_size;
      182:  192:        p->slab_list = new_list;
        -:  193:    }
        -:  194:    return 1;
        -:  195:}
        -:  196:
        3:  197:static void split_slab_page_into_freelist(char *ptr, const unsigned int id) {
   601087:  198:    slabclass_t *p = &slabclass[id];
        -:  199:    int x;
    11021:  200:    for (x = 0; x < p->perslab; x++) {
   600347:  201:        do_slabs_free(ptr, 0, id);
   600347:  202:        ptr += p->size;
        -:  203:    }
        -:  204:}
        -:  205:
        -:  206:/* Fast FIFO queue */
      747:  207:static void *get_page_from_global_pool(void) {
      747:  208:    slabclass_t *p = &slabclass[SLAB_GLOBAL_PAGE_POOL];
      747:  209:    if (p->slabs < 1) {
        -:  210:        return NULL;
        -:  211:    }
       88:  212:    char *ret = p->slab_list[p->slabs - 1];
       88:  213:    p->slabs--;
       88:  214:    return ret;
        -:  215:}
        -:  216:
    17722:  217:static int do_slabs_newslab(const unsigned int id) {
    17722:  218:    slabclass_t *p = &slabclass[id];
    17722:  219:    slabclass_t *g = &slabclass[SLAB_GLOBAL_PAGE_POOL];
     8236:  220:    int len = (settings.slab_reassign || settings.slab_chunk_size_max != settings.slab_page_size)
        -:  221:        ? settings.slab_page_size
    25958:  222:        : p->size * p->perslab;
    17722:  223:    char *ptr;
        -:  224:
    17722:  225:    if ((mem_limit && mem_malloced + len > mem_limit && p->slabs > 0
    17043:  226:         && g->slabs == 0)) {
    16986:  227:        mem_limit_reached = true;
    16986:  228:        MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);
    16986:  229:        return 0;
        -:  230:    }
        -:  231:
      736:  232:    if ((grow_slab_list(id) == 0) ||
       83:  233:        (((ptr = get_page_from_global_pool()) == NULL) &&
      654:  234:        ((ptr = memory_allocate((size_t)len)) == 0))) {
        -:  235:
    #####:  236:        MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);
    #####:  237:        return 0;
        -:  238:    }
        -:  239:
      737:  240:    memset(ptr, 0, (size_t)len);
   590066:  241:    split_slab_page_into_freelist(ptr, id);
        -:  242:
      737:  243:    p->slab_list[p->slabs++] = ptr;
      737:  244:    MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id);
        -:  245:
      737:  246:    return 1;
        -:  247:}
        -:  248:
        -:  249:/* This calculation ends up adding sizeof(void *) to the item size. */
    19806:  250:static void *do_slabs_alloc_chunked(const size_t size, slabclass_t *p, unsigned int id) {
    19806:  251:    void *ret = NULL;
    19806:  252:    item *it = NULL;
    19806:  253:    int x;
    19806:  254:    int csize = p->size - sizeof(item_chunk);
    19806:  255:    unsigned int chunks_req = size / csize;
    19806:  256:    if (size % csize != 0)
    19801:  257:        chunks_req++;
    19915:  258:    while (p->sl_curr < chunks_req) {
      578:  259:        if (do_slabs_newslab(id) == 0)
        -:  260:            break;
        -:  261:    }
        -:  262:
    19806:  263:    if (p->sl_curr >= chunks_req) {
    19337:  264:        item_chunk *chunk = NULL;
        -:  265:
        -:  266:        /* Configure the head item in the chain. */
    19337:  267:        it = (item *)p->slots;
    19337:  268:        p->slots = it->next;
    19337:  269:        if (it->next) it->next->prev = 0;
        -:  270:
        -:  271:        /* Squirrel away the "top chunk" into h_next for now */
    19337:  272:        it->h_next = (item *)p->slots;
   19337*:  273:        assert(it->h_next != 0);
        -:  274:        chunk = (item_chunk *) it->h_next;
        -:  275:
        -:  276:        /* roll down the chunks, marking them as such. */
    95169:  277:        for (x = 0; x < chunks_req-1; x++) {
    75832:  278:            chunk->it_flags &= ~ITEM_SLABBED;
    75832:  279:            chunk->it_flags |= ITEM_CHUNK;
        -:  280:            /* Chunks always have a direct reference to the head item */
    75832:  281:            chunk->head = it;
    75832:  282:            chunk->size = p->size - sizeof(item_chunk);
    75832:  283:            chunk->used = 0;
    75832:  284:            chunk = chunk->next;
        -:  285:        }
        -:  286:
        -:  287:        /* The final "next" is now the top of the slab freelist */
    19337:  288:        p->slots = chunk;
    19337:  289:        if (chunk && chunk->prev) {
        -:  290:            /* Disconnect the final chunk from the chain */
    19296:  291:            chunk->prev->next = 0;
    19296:  292:            chunk->prev = 0;
        -:  293:        }
        -:  294:
    19337:  295:        it->it_flags &= ~ITEM_SLABBED;
    19337:  296:        it->it_flags |= ITEM_CHUNKED;
    19337:  297:        it->refcount = 1;
    19337:  298:        p->sl_curr -= chunks_req;
    19337:  299:        ret = (void *)it;
        -:  300:    } else {
        -:  301:        ret = NULL;
        -:  302:    }
        -:  303:
    19806:  304:    return ret;
        -:  305:}
        -:  306:
        -:  307:/*@null@*/
   138374:  308:static void *do_slabs_alloc(const size_t size, unsigned int id, uint64_t *total_bytes,
        -:  309:        unsigned int flags) {
   138374:  310:    slabclass_t *p;
   138374:  311:    void *ret = NULL;
   138374:  312:    item *it = NULL;
        -:  313:
   138374:  314:    if (id < POWER_SMALLEST || id > power_largest) {
        -:  315:        MEMCACHED_SLABS_ALLOCATE_FAILED(size, 0);
        -:  316:        return NULL;
        -:  317:    }
   138374:  318:    p = &slabclass[id];
  138374*:  319:    assert(p->sl_curr == 0 || ((item *)p->slots)->slabs_clsid == 0);
   138374:  320:    if (total_bytes != NULL) {
   135712:  321:        *total_bytes = p->requested;
        -:  322:    }
        -:  323:
   138374:  324:    if (size <= p->size) {
        -:  325:        /* fail unless we have space at the end of a recently allocated page,
        -:  326:           we have something on our freelist, or we could allocate a new page */
   118568:  327:        if (p->sl_curr == 0 && flags != SLABS_ALLOC_NO_NEWPAGE) {
    17145:  328:            do_slabs_newslab(id);
        -:  329:        }
        -:  330:
   118568:  331:        if (p->sl_curr != 0) {
        -:  332:            /* return off our freelist */
   101643:  333:            it = (item *)p->slots;
   101643:  334:            p->slots = it->next;
   101643:  335:            if (it->next) it->next->prev = 0;
        -:  336:            /* Kill flag and initialize refcount here for lock safety in slab
        -:  337:             * mover's freeness detection. */
   101643:  338:            it->it_flags &= ~ITEM_SLABBED;
   101643:  339:            it->refcount = 1;
   101643:  340:            p->sl_curr--;
   101643:  341:            ret = (void *)it;
        -:  342:        } else {
        -:  343:            ret = NULL;
        -:  344:        }
        -:  345:    } else {
        -:  346:        /* Dealing with a chunked item. */
    19806:  347:        ret = do_slabs_alloc_chunked(size, p, id);
        -:  348:    }
        -:  349:
   121449:  350:    if (ret) {
   120980:  351:        p->requested += size;
   120980:  352:        MEMCACHED_SLABS_ALLOCATE(size, id, p->size, ret);
        -:  353:    } else {
        -:  354:        MEMCACHED_SLABS_ALLOCATE_FAILED(size, id);
        -:  355:    }
        -:  356:
        -:  357:    return ret;
        -:  358:}
        -:  359:
        -:  360:static void do_slabs_free_chunked(item *it, const size_t size, unsigned int id,
        -:  361:                                  slabclass_t *p) {
        -:  362:    item_chunk *chunk = (item_chunk *) ITEM_data(it);
        -:  363:    size_t realsize = size;
        -:  364:    while (chunk) {
        -:  365:        realsize += sizeof(item_chunk);
        -:  366:        chunk = chunk->next;
        -:  367:    }
        -:  368:    chunk = (item_chunk *) ITEM_data(it);
        -:  369:    unsigned int chunks_found = 1;
        -:  370:
        -:  371:    it->it_flags = ITEM_SLABBED;
        -:  372:    it->slabs_clsid = 0;
        -:  373:    it->prev = 0;
        -:  374:    it->next = (item *) chunk->next;
        -:  375:    assert(it->next);
        -:  376:    /* top chunk should already point back to head */
        -:  377:    assert(it->next && (void*)it->next->prev == (void*)chunk);
        -:  378:    chunk = chunk->next;
        -:  379:    chunk->prev = (item_chunk *)it;
        -:  380:
        -:  381:    while (chunk) {
        -:  382:        assert(chunk->it_flags == ITEM_CHUNK);
        -:  383:        chunk->it_flags = ITEM_SLABBED;
        -:  384:        chunk->slabs_clsid = 0;
        -:  385:        chunks_found++;
        -:  386:        if (chunk->next) {
        -:  387:            chunk = chunk->next;
        -:  388:        } else {
        -:  389:            break;
        -:  390:        }
        -:  391:    }
        -:  392:    /* must have had nothing hanging off of the final chunk */
        -:  393:    assert(chunk && chunk->next == 0);
        -:  394:    /* Tail chunk, link the freelist here. */
        -:  395:    chunk->next = p->slots;
        -:  396:    if (chunk->next) chunk->next->prev = chunk;
        -:  397:
        -:  398:    p->slots = it;
        -:  399:    p->sl_curr += chunks_found;
        -:  400:    p->requested -= size;
        -:  401:
        -:  402:    return;
        -:  403:}
        -:  404:
        -:  405:
   679689:  406:static void do_slabs_free(void *ptr, const size_t size, unsigned int id) {
   679689:  407:    slabclass_t *p;
   679689:  408:    item *it;
        -:  409:
  679689*:  410:    assert(id >= POWER_SMALLEST && id <= power_largest);
   679689:  411:    if (id < POWER_SMALLEST || id > power_largest)
        -:  412:        return;
        -:  413:
   679689:  414:    MEMCACHED_SLABS_FREE(size, id, ptr);
   679689:  415:    p = &slabclass[id];
        -:  416:
   679689:  417:    it = (item *)ptr;
   679689:  418:    if ((it->it_flags & ITEM_CHUNKED) == 0) {
   663822:  419:        it->it_flags = ITEM_SLABBED;
   663822:  420:        it->slabs_clsid = 0;
   663822:  421:        it->prev = 0;
   663822:  422:        it->next = p->slots;
   663822:  423:        if (it->next) it->next->prev = it;
   663822:  424:        p->slots = it;
        -:  425:
   663822:  426:        p->sl_curr++;
   663822:  427:        p->requested -= size;
        -:  428:    } else {
    15867:  429:        do_slabs_free_chunked(it, size, id, p);
        -:  430:    }
        -:  431:    return;
        -:  432:}
        -:  433:
       59:  434:static int nz_strcmp(int nzlength, const char *nz, const char *z) {
       59:  435:    int zlength=strlen(z);
       59:  436:    return (zlength == nzlength) && (strncmp(nz, z, zlength) == 0) ? 0 : -1;
        -:  437:}
        -:  438:
     2806:  439:bool get_stats(const char *stat_type, int nkey, ADD_STAT add_stats, void *c) {
     2806:  440:    bool ret = true;
        -:  441:
     2806:  442:    if (add_stats != NULL) {
     2806:  443:        if (!stat_type) {
        -:  444:            /* prepare general statistics for the engine */
     2770:  445:            STATS_LOCK();
     2770:  446:            APPEND_STAT("bytes", "%llu", (unsigned long long)stats_state.curr_bytes);
     2770:  447:            APPEND_STAT("curr_items", "%llu", (unsigned long long)stats_state.curr_items);
     2770:  448:            APPEND_STAT("total_items", "%llu", (unsigned long long)stats.total_items);
     2770:  449:            STATS_UNLOCK();
     2770:  450:            if (settings.slab_automove > 0) {
       15:  451:                pthread_mutex_lock(&slabs_lock);
       15:  452:                APPEND_STAT("slab_global_page_pool", "%u", slabclass[SLAB_GLOBAL_PAGE_POOL].slabs);
       15:  453:                pthread_mutex_unlock(&slabs_lock);
        -:  454:            }
     2770:  455:            item_stats_totals(add_stats, c);
       36:  456:        } else if (nz_strcmp(nkey, stat_type, "items") == 0) {
       13:  457:            item_stats(add_stats, c);
       23:  458:        } else if (nz_strcmp(nkey, stat_type, "slabs") == 0) {
       23:  459:            slabs_stats(add_stats, c);
    #####:  460:        } else if (nz_strcmp(nkey, stat_type, "sizes") == 0) {
    #####:  461:            item_stats_sizes(add_stats, c);
    #####:  462:        } else if (nz_strcmp(nkey, stat_type, "sizes_enable") == 0) {
    #####:  463:            item_stats_sizes_enable(add_stats, c);
    #####:  464:        } else if (nz_strcmp(nkey, stat_type, "sizes_disable") == 0) {
    #####:  465:            item_stats_sizes_disable(add_stats, c);
        -:  466:        } else {
        -:  467:            ret = false;
        -:  468:        }
        -:  469:    } else {
        -:  470:        ret = false;
        -:  471:    }
        -:  472:
     2806:  473:    return ret;
        -:  474:}
        -:  475:
        -:  476:/*@null@*/
       23:  477:static void do_slabs_stats(ADD_STAT add_stats, void *c) {
       23:  478:    int i, total;
        -:  479:    /* Get the per-thread stats which contain some interesting aggregates */
       23:  480:    struct thread_stats thread_stats;
       23:  481:    threadlocal_stats_aggregate(&thread_stats);
        -:  482:
       23:  483:    total = 0;
      975:  484:    for(i = POWER_SMALLEST; i <= power_largest; i++) {
      929:  485:        slabclass_t *p = &slabclass[i];
      929:  486:        if (p->slabs != 0) {
       71:  487:            uint32_t perslab, slabs;
       71:  488:            slabs = p->slabs;
       71:  489:            perslab = p->perslab;
        -:  490:
       71:  491:            char key_str[STAT_KEY_LEN];
       71:  492:            char val_str[STAT_VAL_LEN];
       71:  493:            int klen = 0, vlen = 0;
        -:  494:
       71:  495:            APPEND_NUM_STAT(i, "chunk_size", "%u", p->size);
       71:  496:            APPEND_NUM_STAT(i, "chunks_per_page", "%u", perslab);
       71:  497:            APPEND_NUM_STAT(i, "total_pages", "%u", slabs);
       71:  498:            APPEND_NUM_STAT(i, "total_chunks", "%u", slabs * perslab);
       71:  499:            APPEND_NUM_STAT(i, "used_chunks", "%u",
       71:  500:                            slabs*perslab - p->sl_curr);
       71:  501:            APPEND_NUM_STAT(i, "free_chunks", "%u", p->sl_curr);
        -:  502:            /* Stat is dead, but displaying zero instead of removing it. */
       71:  503:            APPEND_NUM_STAT(i, "free_chunks_end", "%u", 0);
       71:  504:            APPEND_NUM_STAT(i, "mem_requested", "%llu",
       71:  505:                            (unsigned long long)p->requested);
       71:  506:            APPEND_NUM_STAT(i, "get_hits", "%llu",
       71:  507:                    (unsigned long long)thread_stats.slab_stats[i].get_hits);
       71:  508:            APPEND_NUM_STAT(i, "cmd_set", "%llu",
       71:  509:                    (unsigned long long)thread_stats.slab_stats[i].set_cmds);
       71:  510:            APPEND_NUM_STAT(i, "delete_hits", "%llu",
       71:  511:                    (unsigned long long)thread_stats.slab_stats[i].delete_hits);
       71:  512:            APPEND_NUM_STAT(i, "incr_hits", "%llu",
       71:  513:                    (unsigned long long)thread_stats.slab_stats[i].incr_hits);
       71:  514:            APPEND_NUM_STAT(i, "decr_hits", "%llu",
       71:  515:                    (unsigned long long)thread_stats.slab_stats[i].decr_hits);
       71:  516:            APPEND_NUM_STAT(i, "cas_hits", "%llu",
       71:  517:                    (unsigned long long)thread_stats.slab_stats[i].cas_hits);
       71:  518:            APPEND_NUM_STAT(i, "cas_badval", "%llu",
       71:  519:                    (unsigned long long)thread_stats.slab_stats[i].cas_badval);
       71:  520:            APPEND_NUM_STAT(i, "touch_hits", "%llu",
       71:  521:                    (unsigned long long)thread_stats.slab_stats[i].touch_hits);
       71:  522:            total++;
        -:  523:        }
        -:  524:    }
        -:  525:
        -:  526:    /* add overall slab stats and append terminator */
        -:  527:
       23:  528:    APPEND_STAT("active_slabs", "%d", total);
       23:  529:    APPEND_STAT("total_malloced", "%llu", (unsigned long long)mem_malloced);
       23:  530:    add_stats(NULL, 0, NULL, 0, c);
       23:  531:}
        -:  532:
      654:  533:static void *memory_allocate(size_t size) {
      654:  534:    void *ret;
        -:  535:
      654:  536:    if (mem_base == NULL) {
        -:  537:        /* We are not using a preallocated large memory chunk */
      654:  538:        ret = malloc(size);
        -:  539:    } else {
    #####:  540:        ret = mem_current;
        -:  541:
    #####:  542:        if (size > mem_avail) {
        -:  543:            return NULL;
        -:  544:        }
        -:  545:
        -:  546:        /* mem_current pointer _must_ be aligned!!! */
    #####:  547:        if (size % CHUNK_ALIGN_BYTES) {
    #####:  548:            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);
        -:  549:        }
        -:  550:
    #####:  551:        mem_current = ((char*)mem_current) + size;
    #####:  552:        if (size < mem_avail) {
    #####:  553:            mem_avail -= size;
        -:  554:        } else {
    #####:  555:            mem_avail = 0;
        -:  556:        }
        -:  557:    }
      654:  558:    mem_malloced += size;
        -:  559:
      654:  560:    return ret;
        -:  561:}
        -:  562:
        -:  563:/* Must only be used if all pages are item_size_max */
       97:  564:static void memory_release() {
       97:  565:    void *p = NULL;
       97:  566:    if (mem_base != NULL)
        -:  567:        return;
        -:  568:
       97:  569:    if (!settings.slab_reassign)
        -:  570:        return;
        -:  571:
      102:  572:    while (mem_malloced > mem_limit &&
        5:  573:            (p = get_page_from_global_pool()) != NULL) {
        5:  574:        free(p);
        5:  575:        mem_malloced -= settings.item_size_max;
        -:  576:    }
        -:  577:}
        -:  578:
   135711:  579:void *slabs_alloc(size_t size, unsigned int id, uint64_t *total_bytes,
        -:  580:        unsigned int flags) {
   135711:  581:    void *ret;
        -:  582:
   135711:  583:    pthread_mutex_lock(&slabs_lock);
   135711:  584:    ret = do_slabs_alloc(size, id, total_bytes, flags);
   135711:  585:    pthread_mutex_unlock(&slabs_lock);
   135711:  586:    return ret;
        -:  587:}
        -:  588:
    79341:  589:void slabs_free(void *ptr, size_t size, unsigned int id) {
    79341:  590:    pthread_mutex_lock(&slabs_lock);
    79341:  591:    do_slabs_free(ptr, size, id);
    79341:  592:    pthread_mutex_unlock(&slabs_lock);
    79341:  593:}
        -:  594:
       23:  595:void slabs_stats(ADD_STAT add_stats, void *c) {
       23:  596:    pthread_mutex_lock(&slabs_lock);
       23:  597:    do_slabs_stats(add_stats, c);
       23:  598:    pthread_mutex_unlock(&slabs_lock);
       23:  599:}
        -:  600:
        3:  601:static bool do_slabs_adjust_mem_limit(size_t new_mem_limit) {
        -:  602:    /* Cannot adjust memory limit at runtime if prealloc'ed */
        3:  603:    if (mem_base != NULL)
        -:  604:        return false;
        3:  605:    settings.maxbytes = new_mem_limit;
        3:  606:    mem_limit = new_mem_limit;
        3:  607:    mem_limit_reached = false; /* Will reset on next alloc */
        3:  608:    memory_release(); /* free what might already be in the global pool */
        3:  609:    return true;
        -:  610:}
        -:  611:
        3:  612:bool slabs_adjust_mem_limit(size_t new_mem_limit) {
        3:  613:    bool ret;
        3:  614:    pthread_mutex_lock(&slabs_lock);
        3:  615:    ret = do_slabs_adjust_mem_limit(new_mem_limit);
        3:  616:    pthread_mutex_unlock(&slabs_lock);
        3:  617:    return ret;
        -:  618:}
        -:  619:
    #####:  620:void slabs_adjust_mem_requested(unsigned int id, size_t old, size_t ntotal)
        -:  621:{
    #####:  622:    pthread_mutex_lock(&slabs_lock);
    #####:  623:    slabclass_t *p;
    #####:  624:    if (id < POWER_SMALLEST || id > power_largest) {
    #####:  625:        fprintf(stderr, "Internal error! Invalid slab class\n");
    #####:  626:        abort();
        -:  627:    }
        -:  628:
    #####:  629:    p = &slabclass[id];
    #####:  630:    p->requested = p->requested - old + ntotal;
    #####:  631:    pthread_mutex_unlock(&slabs_lock);
    #####:  632:}
        -:  633:
    67788:  634:unsigned int slabs_available_chunks(const unsigned int id, bool *mem_flag,
        -:  635:        uint64_t *total_bytes, unsigned int *chunks_perslab) {
    67788:  636:    unsigned int ret;
    67788:  637:    slabclass_t *p;
        -:  638:
    67788:  639:    pthread_mutex_lock(&slabs_lock);
    67788:  640:    p = &slabclass[id];
    67788:  641:    ret = p->sl_curr;
    67788:  642:    if (mem_flag != NULL)
    67788:  643:        *mem_flag = mem_limit_reached;
    67788:  644:    if (total_bytes != NULL)
    67788:  645:        *total_bytes = p->requested;
    67788:  646:    if (chunks_perslab != NULL)
    67788:  647:        *chunks_perslab = p->perslab;
    67788:  648:    pthread_mutex_unlock(&slabs_lock);
    67788:  649:    return ret;
        -:  650:}
        -:  651:
        -:  652:static pthread_cond_t slab_rebalance_cond = PTHREAD_COND_INITIALIZER;
        -:  653:static volatile int do_run_slab_thread = 1;
        -:  654:static volatile int do_run_slab_rebalance_thread = 1;
        -:  655:
        -:  656:#define DEFAULT_SLAB_BULK_CHECK 1
        -:  657:int slab_bulk_check = DEFAULT_SLAB_BULK_CHECK;
        -:  658:
       97:  659:static int slab_rebalance_start(void) {
       97:  660:    slabclass_t *s_cls;
       97:  661:    int no_go = 0;
        -:  662:
       97:  663:    pthread_mutex_lock(&slabs_lock);
        -:  664:
       97:  665:    if (slab_rebal.s_clsid < POWER_SMALLEST ||
       97:  666:        slab_rebal.s_clsid > power_largest  ||
       97:  667:        slab_rebal.d_clsid < SLAB_GLOBAL_PAGE_POOL ||
       97:  668:        slab_rebal.d_clsid > power_largest  ||
        -:  669:        slab_rebal.s_clsid == slab_rebal.d_clsid)
    #####:  670:        no_go = -2;
        -:  671:
       97:  672:    s_cls = &slabclass[slab_rebal.s_clsid];
        -:  673:
       97:  674:    if (!grow_slab_list(slab_rebal.d_clsid)) {
    #####:  675:        no_go = -1;
        -:  676:    }
        -:  677:
       97:  678:    if (s_cls->slabs < 2)
        -:  679:        no_go = -3;
        -:  680:
       97:  681:    if (no_go != 0) {
    #####:  682:        pthread_mutex_unlock(&slabs_lock);
    #####:  683:        return no_go; /* Should use a wrapper function... */
        -:  684:    }
        -:  685:
        -:  686:    /* Always kill the first available slab page as it is most likely to
        -:  687:     * contain the oldest items
        -:  688:     */
       97:  689:    slab_rebal.slab_start = s_cls->slab_list[0];
       97:  690:    slab_rebal.slab_end   = (char *)slab_rebal.slab_start +
       97:  691:        (s_cls->size * s_cls->perslab);
       97:  692:    slab_rebal.slab_pos   = slab_rebal.slab_start;
       97:  693:    slab_rebal.done       = 0;
        -:  694:
        -:  695:    /* Also tells do_item_get to search for items in this slab */
       97:  696:    slab_rebalance_signal = 2;
        -:  697:
       97:  698:    if (settings.verbose > 1) {
    #####:  699:        fprintf(stderr, "Started a slab rebalance\n");
        -:  700:    }
        -:  701:
       97:  702:    pthread_mutex_unlock(&slabs_lock);
        -:  703:
       97:  704:    STATS_LOCK();
       97:  705:    stats_state.slab_reassign_running = true;
       97:  706:    STATS_UNLOCK();
        -:  707:
       97:  708:    return 0;
        -:  709:}
        -:  710:
        -:  711:/* CALLED WITH slabs_lock HELD */
     2262:  712:static void *slab_rebalance_alloc(const size_t size, unsigned int id) {
     2262:  713:    slabclass_t *s_cls;
     2262:  714:    s_cls = &slabclass[slab_rebal.s_clsid];
     2262:  715:    int x;
     2262:  716:    item *new_it = NULL;
        -:  717:
     2662:  718:    for (x = 0; x < s_cls->perslab; x++) {
     2662:  719:        new_it = do_slabs_alloc(size, id, NULL, SLABS_ALLOC_NO_NEWPAGE);
        -:  720:        /* check that memory isn't within the range to clear */
     2662:  721:        if (new_it == NULL) {
        -:  722:            break;
        -:  723:        }
     2254:  724:        if ((void *)new_it >= slab_rebal.slab_start
      400:  725:            && (void *)new_it < slab_rebal.slab_end) {
        -:  726:            /* Pulled something we intend to free. Mark it as freed since
        -:  727:             * we've already done the work of unlinking it from the freelist.
        -:  728:             */
      400:  729:            s_cls->requested -= size;
      400:  730:            new_it->refcount = 0;
      400:  731:            new_it->it_flags = ITEM_SLABBED|ITEM_FETCHED;
        -:  732:#ifdef DEBUG_SLAB_MOVER
        -:  733:            memcpy(ITEM_key(new_it), "deadbeef", 8);
        -:  734:#endif
      400:  735:            new_it = NULL;
      400:  736:            slab_rebal.inline_reclaim++;
        -:  737:        } else {
        -:  738:            break;
        -:  739:        }
        -:  740:    }
     2262:  741:    return new_it;
        -:  742:}
        -:  743:
        -:  744:/* CALLED WITH slabs_lock HELD */
        -:  745:/* detatches item/chunk from freelist. */
        -:  746:static void slab_rebalance_cut_free(slabclass_t *s_cls, item *it) {
        -:  747:    /* Ensure this was on the freelist and nothing else. */
        -:  748:    assert(it->it_flags == ITEM_SLABBED);
        -:  749:    if (s_cls->slots == it) {
        -:  750:        s_cls->slots = it->next;
        -:  751:    }
        -:  752:    if (it->next) it->next->prev = it->prev;
        -:  753:    if (it->prev) it->prev->next = it->next;
        -:  754:    s_cls->sl_curr--;
        -:  755:}
        -:  756:
        -:  757:enum move_status {
        -:  758:    MOVE_PASS=0, MOVE_FROM_SLAB, MOVE_FROM_LRU, MOVE_BUSY, MOVE_LOCKED
        -:  759:};
        -:  760:
        -:  761:/* refcount == 0 is safe since nobody can incr while item_lock is held.
        -:  762: * refcount != 0 is impossible since flags/etc can be modified in other
        -:  763: * threads. instead, note we found a busy one and bail. logic in do_item_get
        -:  764: * will prevent busy items from continuing to be busy
        -:  765: * NOTE: This is checking it_flags outside of an item lock. I believe this
        -:  766: * works since it_flags is 8 bits, and we're only ever comparing a single bit
        -:  767: * regardless. ITEM_SLABBED bit will always be correct since we're holding the
        -:  768: * lock which modifies that bit. ITEM_LINKED won't exist if we're between an
        -:  769: * item having ITEM_SLABBED removed, and the key hasn't been added to the item
        -:  770: * yet. The memory barrier from the slabs lock should order the key write and the
        -:  771: * flags to the item?
        -:  772: * If ITEM_LINKED did exist and was just removed, but we still see it, that's
        -:  773: * still safe since it will have a valid key, which we then lock, and then
        -:  774: * recheck everything.
        -:  775: * This may not be safe on all platforms; If not, slabs_alloc() will need to
        -:  776: * seed the item key while holding slabs_lock.
        -:  777: */
    15598:  778:static int slab_rebalance_move(void) {
    15598:  779:    slabclass_t *s_cls;
    15598:  780:    int x;
    15598:  781:    int was_busy = 0;
    15598:  782:    int refcount = 0;
    15598:  783:    uint32_t hv;
    15598:  784:    void *hold_lock;
    15598:  785:    enum move_status status = MOVE_PASS;
        -:  786:
    15598:  787:    pthread_mutex_lock(&slabs_lock);
        -:  788:
    15598:  789:    s_cls = &slabclass[slab_rebal.s_clsid];
        -:  790:
    30711:  791:    for (x = 0; x < slab_bulk_check; x++) {
    15598:  792:        hv = 0;
    15598:  793:        hold_lock = NULL;
    15598:  794:        item *it = slab_rebal.slab_pos;
    15598:  795:        item_chunk *ch = NULL;
    15598:  796:        status = MOVE_PASS;
    15598:  797:        if (it->it_flags & ITEM_CHUNK) {
        -:  798:            /* This chunk is a chained part of a larger item. */
      427:  799:            ch = (item_chunk *) it;
        -:  800:            /* Instead, we use the head chunk to find the item and effectively
        -:  801:             * lock the entire structure. If a chunk has ITEM_CHUNK flag, its
        -:  802:             * head cannot be slabbed, so the normal routine is safe. */
      427:  803:            it = ch->head;
     427*:  804:            assert(it->it_flags & ITEM_CHUNKED);
        -:  805:        }
        -:  806:
        -:  807:        /* ITEM_FETCHED when ITEM_SLABBED is overloaded to mean we've cleared
        -:  808:         * the chunk for move. Only these two flags should exist.
        -:  809:         */
    15598:  810:        if (it->it_flags != (ITEM_SLABBED|ITEM_FETCHED)) {
        -:  811:            /* ITEM_SLABBED can only be added/removed under the slabs_lock */
    10464:  812:            if (it->it_flags & ITEM_SLABBED) {
    7821*:  813:                assert(ch == NULL);
     7821:  814:                slab_rebalance_cut_free(s_cls, it);
     7821:  815:                status = MOVE_FROM_SLAB;
     2643:  816:            } else if ((it->it_flags & ITEM_LINKED) != 0) {
        -:  817:                /* If it doesn't have ITEM_SLABBED, the item could be in any
        -:  818:                 * state on its way to being freed or written to. If no
        -:  819:                 * ITEM_SLABBED, but it's had ITEM_LINKED, it must be active
        -:  820:                 * and have the key written to it already.
        -:  821:                 */
     2262:  822:                hv = hash(ITEM_key(it), it->nkey);
     2262:  823:                if ((hold_lock = item_trylock(hv)) == NULL) {
        -:  824:                    status = MOVE_LOCKED;
        -:  825:                } else {
     2262:  826:                    refcount = refcount_incr(&it->refcount);
     2262:  827:                    if (refcount == 2) { /* item is linked but not busy */
        -:  828:                        /* Double check ITEM_LINKED flag here, since we're
        -:  829:                         * past a memory barrier from the mutex. */
     2262:  830:                        if ((it->it_flags & ITEM_LINKED) != 0) {
        -:  831:                            status = MOVE_FROM_LRU;
        -:  832:                        } else {
        -:  833:                            /* refcount == 1 + !ITEM_LINKED means the item is being
        -:  834:                             * uploaded to, or was just unlinked but hasn't been freed
        -:  835:                             * yet. Let it bleed off on its own and try again later */
        -:  836:                            status = MOVE_BUSY;
        -:  837:                        }
        -:  838:                    } else {
    #####:  839:                        if (settings.verbose > 2) {
    #####:  840:                            fprintf(stderr, "Slab reassign hit a busy item: refcount: %d (%d -> %d)\n",
    #####:  841:                                it->refcount, slab_rebal.s_clsid, slab_rebal.d_clsid);
        -:  842:                        }
        -:  843:                        status = MOVE_BUSY;
        -:  844:                    }
        -:  845:                    /* Item lock must be held while modifying refcount */
        -:  846:                    if (status == MOVE_BUSY) {
    #####:  847:                        refcount_decr(&it->refcount);
    #####:  848:                        item_trylock_unlock(hold_lock);
        -:  849:                    }
        -:  850:                }
        -:  851:            } else {
        -:  852:                /* See above comment. No ITEM_SLABBED or ITEM_LINKED. Mark
        -:  853:                 * busy and wait for item to complete its upload. */
        -:  854:                status = MOVE_BUSY;
        -:  855:            }
        -:  856:        }
        -:  857:
    15598:  858:        int save_item = 0;
    15598:  859:        item *new_it = NULL;
    15598:  860:        size_t ntotal = 0;
    15598:  861:        switch (status) {
     2262:  862:            case MOVE_FROM_LRU:
        -:  863:                /* Lock order is LRU locks -> slabs_lock. unlink uses LRU lock.
        -:  864:                 * We only need to hold the slabs_lock while initially looking
        -:  865:                 * at an item, and at this point we have an exclusive refcount
        -:  866:                 * (2) + the item is locked. Drop slabs lock, drop item to
        -:  867:                 * refcount 1 (just our own, then fall through and wipe it
        -:  868:                 */
        -:  869:                /* Check if expired or flushed */
     2262:  870:                ntotal = ITEM_ntotal(it);
        -:  871:                /* REQUIRES slabs_lock: CHECK FOR cls->sl_curr > 0 */
     2262:  872:                if (ch == NULL && (it->it_flags & ITEM_CHUNKED)) {
        -:  873:                    /* Chunked should be identical to non-chunked, except we need
        -:  874:                     * to swap out ntotal for the head-chunk-total. */
       86:  875:                    ntotal = s_cls->size;
        -:  876:                }
    2262*:  877:                if ((it->exptime != 0 && it->exptime < current_time)
     2262:  878:                    || item_is_flushed(it)) {
        -:  879:                    /* Expired, don't save. */
        -:  880:                    save_item = 0;
     2262:  881:                } else if (ch == NULL &&
     1835:  882:                        (new_it = slab_rebalance_alloc(ntotal, slab_rebal.s_clsid)) == NULL) {
        -:  883:                    /* Not a chunk of an item, and nomem. */
      238:  884:                    save_item = 0;
      238:  885:                    slab_rebal.evictions_nomem++;
     2024:  886:                } else if (ch != NULL &&
      427:  887:                        (new_it = slab_rebalance_alloc(s_cls->size, slab_rebal.s_clsid)) == NULL) {
        -:  888:                    /* Is a chunk of an item, and nomem. */
      170:  889:                    save_item = 0;
      170:  890:                    slab_rebal.evictions_nomem++;
        -:  891:                } else {
        -:  892:                    /* Was whatever it was, and we have memory for it. */
        -:  893:                    save_item = 1;
        -:  894:                }
     2262:  895:                pthread_mutex_unlock(&slabs_lock);
     2262:  896:                unsigned int requested_adjust = 0;
     2262:  897:                if (save_item) {
     1854:  898:                    if (ch == NULL) {
    1597*:  899:                        assert((new_it->it_flags & ITEM_CHUNKED) == 0);
        -:  900:                        /* if free memory, memcpy. clear prev/next/h_bucket */
     1597:  901:                        memcpy(new_it, it, ntotal);
     1597:  902:                        new_it->prev = 0;
     1597:  903:                        new_it->next = 0;
     1597:  904:                        new_it->h_next = 0;
        -:  905:                        /* These are definitely required. else fails assert */
     1597:  906:                        new_it->it_flags &= ~ITEM_LINKED;
     1597:  907:                        new_it->refcount = 0;
     1597:  908:                        do_item_replace(it, new_it, hv);
        -:  909:                        /* Need to walk the chunks and repoint head  */
     1597:  910:                        if (new_it->it_flags & ITEM_CHUNKED) {
       20:  911:                            item_chunk *fch = (item_chunk *) ITEM_data(new_it);
       20:  912:                            fch->next->prev = fch;
       80:  913:                            while (fch) {
       60:  914:                                fch->head = new_it;
       60:  915:                                fch = fch->next;
        -:  916:                            }
        -:  917:                        }
     1597:  918:                        it->refcount = 0;
     1597:  919:                        it->it_flags = ITEM_SLABBED|ITEM_FETCHED;
        -:  920:#ifdef DEBUG_SLAB_MOVER
        -:  921:                        memcpy(ITEM_key(it), "deadbeef", 8);
        -:  922:#endif
     1597:  923:                        slab_rebal.rescues++;
     1597:  924:                        requested_adjust = ntotal;
        -:  925:                    } else {
      257:  926:                        item_chunk *nch = (item_chunk *) new_it;
        -:  927:                        /* Chunks always have head chunk (the main it) */
      257:  928:                        ch->prev->next = nch;
      257:  929:                        if (ch->next)
       86:  930:                            ch->next->prev = nch;
      257:  931:                        memcpy(nch, ch, ch->used + sizeof(item_chunk));
      257:  932:                        ch->refcount = 0;
      257:  933:                        ch->it_flags = ITEM_SLABBED|ITEM_FETCHED;
      257:  934:                        slab_rebal.chunk_rescues++;
        -:  935:#ifdef DEBUG_SLAB_MOVER
        -:  936:                        memcpy(ITEM_key((item *)ch), "deadbeef", 8);
        -:  937:#endif
      257:  938:                        refcount_decr(&it->refcount);
      257:  939:                        requested_adjust = s_cls->size;
        -:  940:                    }
        -:  941:                } else {
        -:  942:                    /* restore ntotal in case we tried saving a head chunk. */
      408:  943:                    ntotal = ITEM_ntotal(it);
      408:  944:                    do_item_unlink(it, hv);
      408:  945:                    slabs_free(it, ntotal, slab_rebal.s_clsid);
        -:  946:                    /* Swing around again later to remove it from the freelist. */
      408:  947:                    slab_rebal.busy_items++;
      408:  948:                    was_busy++;
        -:  949:                }
     2262:  950:                item_trylock_unlock(hold_lock);
     2262:  951:                pthread_mutex_lock(&slabs_lock);
        -:  952:                /* Always remove the ntotal, as we added it in during
        -:  953:                 * do_slabs_alloc() when copying the item.
        -:  954:                 */
     2262:  955:                s_cls->requested -= requested_adjust;
     2262:  956:                break;
     7821:  957:            case MOVE_FROM_SLAB:
     7821:  958:                it->refcount = 0;
     7821:  959:                it->it_flags = ITEM_SLABBED|ITEM_FETCHED;
        -:  960:#ifdef DEBUG_SLAB_MOVER
        -:  961:                memcpy(ITEM_key(it), "deadbeef", 8);
        -:  962:#endif
     7821:  963:                break;
      381:  964:            case MOVE_BUSY:
        -:  965:            case MOVE_LOCKED:
      381:  966:                slab_rebal.busy_items++;
      381:  967:                was_busy++;
      381:  968:                break;
        -:  969:            case MOVE_PASS:
        -:  970:                break;
        -:  971:        }
        -:  972:
    15598:  973:        slab_rebal.slab_pos = (char *)slab_rebal.slab_pos + s_cls->size;
    15598:  974:        if (slab_rebal.slab_pos >= slab_rebal.slab_end)
        -:  975:            break;
        -:  976:    }
        -:  977:
    15598:  978:    if (slab_rebal.slab_pos >= slab_rebal.slab_end) {
        -:  979:        /* Some items were busy, start again from the top */
      485:  980:        if (slab_rebal.busy_items) {
      388:  981:            slab_rebal.slab_pos = slab_rebal.slab_start;
      388:  982:            STATS_LOCK();
      388:  983:            stats.slab_reassign_busy_items += slab_rebal.busy_items;
      388:  984:            STATS_UNLOCK();
      388:  985:            slab_rebal.busy_items = 0;
        -:  986:        } else {
       97:  987:            slab_rebal.done++;
        -:  988:        }
        -:  989:    }
        -:  990:
    15598:  991:    pthread_mutex_unlock(&slabs_lock);
        -:  992:
    15598:  993:    return was_busy;
        -:  994:}
        -:  995:
       97:  996:static void slab_rebalance_finish(void) {
       97:  997:    slabclass_t *s_cls;
       97:  998:    slabclass_t *d_cls;
       97:  999:    int x;
       97: 1000:    uint32_t rescues;
       97: 1001:    uint32_t evictions_nomem;
       97: 1002:    uint32_t inline_reclaim;
       97: 1003:    uint32_t chunk_rescues;
        -: 1004:
       97: 1005:    pthread_mutex_lock(&slabs_lock);
        -: 1006:
       97: 1007:    s_cls = &slabclass[slab_rebal.s_clsid];
       97: 1008:    d_cls = &slabclass[slab_rebal.d_clsid];
        -: 1009:
        -: 1010:#ifdef DEBUG_SLAB_MOVER
        -: 1011:    /* If the algorithm is broken, live items can sneak in. */
        -: 1012:    slab_rebal.slab_pos = slab_rebal.slab_start;
        -: 1013:    while (1) {
        -: 1014:        item *it = slab_rebal.slab_pos;
        -: 1015:        assert(it->it_flags == (ITEM_SLABBED|ITEM_FETCHED));
        -: 1016:        assert(memcmp(ITEM_key(it), "deadbeef", 8) == 0);
        -: 1017:        it->it_flags = ITEM_SLABBED|ITEM_FETCHED;
        -: 1018:        slab_rebal.slab_pos = (char *)slab_rebal.slab_pos + s_cls->size;
        -: 1019:        if (slab_rebal.slab_pos >= slab_rebal.slab_end)
        -: 1020:            break;
        -: 1021:    }
        -: 1022:#endif
        -: 1023:
        -: 1024:    /* At this point the stolen slab is completely clear.
        -: 1025:     * We always kill the "first"/"oldest" slab page in the slab_list, so
        -: 1026:     * shuffle the page list backwards and decrement.
        -: 1027:     */
       97: 1028:    s_cls->slabs--;
     2247: 1029:    for (x = 0; x < s_cls->slabs; x++) {
     2150: 1030:        s_cls->slab_list[x] = s_cls->slab_list[x+1];
        -: 1031:    }
        -: 1032:
       97: 1033:    d_cls->slab_list[d_cls->slabs++] = slab_rebal.slab_start;
        -: 1034:    /* Don't need to split the page into chunks if we're just storing it */
       97: 1035:    if (slab_rebal.d_clsid > SLAB_GLOBAL_PAGE_POOL) {
        3: 1036:        memset(slab_rebal.slab_start, 0, (size_t)settings.item_size_max);
      100: 1037:        split_slab_page_into_freelist(slab_rebal.slab_start,
        3: 1038:            slab_rebal.d_clsid);
       94: 1039:    } else if (slab_rebal.d_clsid == SLAB_GLOBAL_PAGE_POOL) {
        -: 1040:        /* mem_malloc'ed might be higher than mem_limit. */
       94: 1041:        memory_release();
        -: 1042:    }
        -: 1043:
       97: 1044:    slab_rebal.done       = 0;
       97: 1045:    slab_rebal.s_clsid    = 0;
       97: 1046:    slab_rebal.d_clsid    = 0;
       97: 1047:    slab_rebal.slab_start = NULL;
       97: 1048:    slab_rebal.slab_end   = NULL;
       97: 1049:    slab_rebal.slab_pos   = NULL;
       97: 1050:    evictions_nomem    = slab_rebal.evictions_nomem;
       97: 1051:    inline_reclaim = slab_rebal.inline_reclaim;
       97: 1052:    rescues   = slab_rebal.rescues;
       97: 1053:    chunk_rescues = slab_rebal.chunk_rescues;
       97: 1054:    slab_rebal.evictions_nomem    = 0;
       97: 1055:    slab_rebal.inline_reclaim = 0;
       97: 1056:    slab_rebal.rescues  = 0;
        -: 1057:
       97: 1058:    slab_rebalance_signal = 0;
        -: 1059:
       97: 1060:    pthread_mutex_unlock(&slabs_lock);
        -: 1061:
       97: 1062:    STATS_LOCK();
       97: 1063:    stats.slabs_moved++;
       97: 1064:    stats.slab_reassign_rescues += rescues;
       97: 1065:    stats.slab_reassign_evictions_nomem += evictions_nomem;
       97: 1066:    stats.slab_reassign_inline_reclaim += inline_reclaim;
       97: 1067:    stats.slab_reassign_chunk_rescues += chunk_rescues;
       97: 1068:    stats_state.slab_reassign_running = false;
       97: 1069:    STATS_UNLOCK();
        -: 1070:
       97: 1071:    if (settings.verbose > 1) {
    #####: 1072:        fprintf(stderr, "finished a slab move\n");
        -: 1073:    }
       97: 1074:}
        -: 1075:
        -: 1076:/* Slab mover thread.
        -: 1077: * Sits waiting for a condition to jump off and shovel some memory about
        -: 1078: */
        4: 1079:static void *slab_rebalance_thread(void *arg) {
        4: 1080:    int was_busy = 0;
        -: 1081:    /* So we first pass into cond_wait with the mutex held */
        4: 1082:    mutex_lock(&slabs_rebalance_lock);
        -: 1083:
    15699: 1084:    while (do_run_slab_rebalance_thread) {
    15699: 1085:        if (slab_rebalance_signal == 1) {
       97: 1086:            if (slab_rebalance_start() < 0) {
        -: 1087:                /* Handle errors with more specifity as required. */
    #####: 1088:                slab_rebalance_signal = 0;
        -: 1089:            }
        -: 1090:
        -: 1091:            was_busy = 0;
    15602: 1092:        } else if (slab_rebalance_signal && slab_rebal.slab_start != NULL) {
    15598: 1093:            was_busy = slab_rebalance_move();
        -: 1094:        }
        -: 1095:
    15699: 1096:        if (slab_rebal.done) {
       97: 1097:            slab_rebalance_finish();
    15602: 1098:        } else if (was_busy) {
        -: 1099:            /* Stuck waiting for some items to unlock, so slow down a bit
        -: 1100:             * to give them a chance to free up */
      789: 1101:            usleep(50);
        -: 1102:        }
        -: 1103:
    15699: 1104:        if (slab_rebalance_signal == 0) {
        -: 1105:            /* always hold this lock while we're running */
      101: 1106:            pthread_cond_wait(&slab_rebalance_cond, &slabs_rebalance_lock);
        -: 1107:        }
        -: 1108:    }
    #####: 1109:    return NULL;
        -: 1110:}
        -: 1111:
        -: 1112:/* Iterate at most once through the slab classes and pick a "random" source.
        -: 1113: * I like this better than calling rand() since rand() is slow enough that we
        -: 1114: * can just check all of the classes once instead.
        -: 1115: */
    #####: 1116:static int slabs_reassign_pick_any(int dst) {
    #####: 1117:    static int cur = POWER_SMALLEST - 1;
    #####: 1118:    int tries = power_largest - POWER_SMALLEST + 1;
    #####: 1119:    for (; tries > 0; tries--) {
    #####: 1120:        cur++;
    #####: 1121:        if (cur > power_largest)
    #####: 1122:            cur = POWER_SMALLEST;
    #####: 1123:        if (cur == dst)
    #####: 1124:            continue;
    #####: 1125:        if (slabclass[cur].slabs > 1) {
        -: 1126:            return cur;
        -: 1127:        }
        -: 1128:    }
        -: 1129:    return -1;
        -: 1130:}
        -: 1131:
      125: 1132:static enum reassign_result_type do_slabs_reassign(int src, int dst) {
      125: 1133:    if (slab_rebalance_signal != 0)
        -: 1134:        return REASSIGN_RUNNING;
        -: 1135:
       97: 1136:    if (src == dst)
        -: 1137:        return REASSIGN_SRC_DST_SAME;
        -: 1138:
        -: 1139:    /* Special indicator to choose ourselves. */
       97: 1140:    if (src == -1) {
    #####: 1141:        src = slabs_reassign_pick_any(dst);
        -: 1142:        /* TODO: If we end up back at -1, return a new error type */
        -: 1143:    }
        -: 1144:
       97: 1145:    if (src < POWER_SMALLEST        || src > power_largest ||
       97: 1146:        dst < SLAB_GLOBAL_PAGE_POOL || dst > power_largest)
        -: 1147:        return REASSIGN_BADCLASS;
        -: 1148:
       97: 1149:    if (slabclass[src].slabs < 2)
        -: 1150:        return REASSIGN_NOSPARE;
        -: 1151:
       97: 1152:    slab_rebal.s_clsid = src;
       97: 1153:    slab_rebal.d_clsid = dst;
        -: 1154:
       97: 1155:    slab_rebalance_signal = 1;
       97: 1156:    pthread_cond_signal(&slab_rebalance_cond);
        -: 1157:
       97: 1158:    return REASSIGN_OK;
        -: 1159:}
        -: 1160:
      125: 1161:enum reassign_result_type slabs_reassign(int src, int dst) {
      125: 1162:    enum reassign_result_type ret;
      125: 1163:    if (pthread_mutex_trylock(&slabs_rebalance_lock) != 0) {
        -: 1164:        return REASSIGN_RUNNING;
        -: 1165:    }
      125: 1166:    ret = do_slabs_reassign(src, dst);
      125: 1167:    pthread_mutex_unlock(&slabs_rebalance_lock);
      125: 1168:    return ret;
        -: 1169:}
        -: 1170:
        -: 1171:/* If we hold this lock, rebalancer can't wake up or move */
    #####: 1172:void slabs_rebalancer_pause(void) {
    #####: 1173:    pthread_mutex_lock(&slabs_rebalance_lock);
    #####: 1174:}
        -: 1175:
    #####: 1176:void slabs_rebalancer_resume(void) {
    #####: 1177:    pthread_mutex_unlock(&slabs_rebalance_lock);
    #####: 1178:}
        -: 1179:
        -: 1180:static pthread_t rebalance_tid;
        -: 1181:
        4: 1182:int start_slab_maintenance_thread(void) {
        4: 1183:    int ret;
        4: 1184:    slab_rebalance_signal = 0;
        4: 1185:    slab_rebal.slab_start = NULL;
        4: 1186:    char *env = getenv("MEMCACHED_SLAB_BULK_CHECK");
        4: 1187:    if (env != NULL) {
    #####: 1188:        slab_bulk_check = atoi(env);
    #####: 1189:        if (slab_bulk_check == 0) {
    #####: 1190:            slab_bulk_check = DEFAULT_SLAB_BULK_CHECK;
        -: 1191:        }
        -: 1192:    }
        -: 1193:
        4: 1194:    if (pthread_cond_init(&slab_rebalance_cond, NULL) != 0) {
    #####: 1195:        fprintf(stderr, "Can't intiialize rebalance condition\n");
    #####: 1196:        return -1;
        -: 1197:    }
        4: 1198:    pthread_mutex_init(&slabs_rebalance_lock, NULL);
        -: 1199:
        4: 1200:    if ((ret = pthread_create(&rebalance_tid, NULL,
        -: 1201:                              slab_rebalance_thread, NULL)) != 0) {
    #####: 1202:        fprintf(stderr, "Can't create rebal thread: %s\n", strerror(ret));
    #####: 1203:        return -1;
        -: 1204:    }
        -: 1205:    return 0;
        -: 1206:}
        -: 1207:
        -: 1208:/* The maintenance thread is on a sleep/loop cycle, so it should join after a
        -: 1209: * short wait */
    #####: 1210:void stop_slab_maintenance_thread(void) {
    #####: 1211:    mutex_lock(&slabs_rebalance_lock);
    #####: 1212:    do_run_slab_thread = 0;
    #####: 1213:    do_run_slab_rebalance_thread = 0;
    #####: 1214:    pthread_cond_signal(&slab_rebalance_cond);
    #####: 1215:    pthread_mutex_unlock(&slabs_rebalance_lock);
        -: 1216:
        -: 1217:    /* Wait for the maintenance thread to stop */
    #####: 1218:    pthread_join(rebalance_tid, NULL);
    #####: 1219:}
